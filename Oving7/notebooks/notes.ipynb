{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Clustering E - M\n",
    "\n",
    "Two types of clustering methods:\n",
    "- Hard clustering: clusters do not overlap\n",
    "  * Each point is a member of exactly one cluster\n",
    "- Soft clustering: Clusters may overlap\n",
    "  * Often generative, probabilistic models\n",
    "\n",
    "### Expectation-maximisation (E-M)\n",
    "\n",
    "1. E-step:\n",
    "    * Compute for each point the probability of being generated by one of the $k$ ($h$ in figures below) components\n",
    "2. M-step:\n",
    "    * Update parameters to maximise the likelihood of the data given those assignments\n",
    "\n",
    "# Kmeans clustering\n",
    "  1) Start with k random points/centers of mass\n",
    "  2) Does an E-step, which assigns all points to the different clusters / points of mass\n",
    "  3) Does an M-step, where it moves the cluster center points towards their different clusters\n",
    "  4) repeat until n - steps / satisfied\n",
    "  <img src=\"05.11-expectation-maximization.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Spectral clustering\n",
    "1) Low-dimensional embedding of data\n",
    "2) K-means in low-dimensional space\n",
    "\n",
    "# Gaussian Mixture Models (GMM)\n",
    "Labels for instances known:\n",
    "1) Estimate the means\n",
    "2) Estimate the variance\n",
    "\n",
    "Labels for instances unknown:\n",
    "1) Suspect there are k sources (Gaussian)\n",
    "2) If we know the parameters of the Gaussians, we can make some interference about the data points\n",
    "3) If we know the labels of the data points, we can make some interference about the parameters of the Gaussians\n",
    "\n",
    "### Singular value decomposition\n",
    "\n",
    "scikit-learn docs: http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html\n",
    "\n",
    "* Factorise the feature matrix $X \\approx U*\\Sigma*V'$\n",
    "* Columns of U, V are left- and right-singular vectors, respectively\n",
    "* Several options after factorisation:\n",
    "    1. Truncate $\\Sigma$ at $k$, yielding a reduced dimensional space $U_k*\\Sigma_k*V_k'$\n",
    "    2. Data imputation: truncated SVD also fills in missing values\n",
    "    3. Examine $U$ or $V'$ for feature-topics and topic-instances\n",
    "\n",
    "SVD topics $U$, $V'$ are difficult to interpret (negative values), so NNMF can be used if this is important.\n",
    "\n",
    "### Non-negative matrix factorisation (NNMF)\n",
    "\n",
    "* Factorise the feature matrix $X = WH$\n",
    "* Inherent clustering of columns of $X$ in $H$ where $H_{kj}$ denotes soft membership of instance $x_j$ to cluster $k$\n",
    "\n",
    "All elements of $W,H$ are >= 0.\n",
    "\n",
    "See this Jupyter Notebook about [NNMF using Tensorflow](https://nipunbatra.github.io/blog/2017/nnmf-tensorflow.html).\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "http://scikit-learn.org/stable/modules/clustering.html#clustering-performance-evaluation\n",
    "\n",
    "Great! We've clustered our data or fit our data to a generative model, is it any good?\n",
    "\n",
    "We want to evaluate the quality of the clusters. This evaluation generally looks at inter-cluster and intra-cluster distances.\n",
    "\n",
    "### Silhouette coefficient (SC)\n",
    "\n",
    "scikit-learn docs: http://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient\n",
    "\n",
    "Simple measure for a hard clustering like k-means. A higher SC means better clusters.\n",
    "\n",
    "Composed of two scores:\n",
    "* a - Mean distance between a sample and all other points in the same class\n",
    "* b - Mean distance between a sample and all other points in the *next nearest* cluster\n",
    "\n",
    "$sc = \\frac{b - a}{max(a, b)}$\n",
    "\n",
    "### Held-out data (imputation)\n",
    "\n",
    "Treat a part of the data as test data (20-25%).\n",
    "\n",
    "1. Build model on training data\n",
    "2. See if the model predicts the test data (use an appropriate error metric)\n",
    "\n",
    "### Evaluation on later models (e.g., classification)\n",
    "\n",
    "Here we assume the unsupervised learning is used to reduce dimensionality or noise before the data is sent to a classifier. In this case we can adjust parameters of the unsupervised model (e.g., $k$), and evaluate on the performance of the classifier, while keeping classifier parameters fixed.\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}